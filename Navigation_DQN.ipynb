{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Project (DQN)\n",
    "---\n",
    "A DQN agent to solve the Navigation Project as part of [Udacity's Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import math \n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from dqn_agent import Agent\n",
    "\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent\n",
    "\n",
    "Initialize the environment in the code cell below. Depending on the operating system, the ``BBANANA_PATH`` to the Unity environment migth need to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "BANANA_PATH = \"Banana.app\"\n",
    "\n",
    "env = UnityEnvironment(file_name=BANANA_PATH,worker_id=1, seed=1) # Otherwise Unity crashes...\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Simplify the Interface of the Environment\n",
    "\n",
    "The environment returns a ``BrainInfo`` object with attributes for the new state, the reward, and other information. The following helper function unpacks the values for ``next_state``, ``reward``, and ``done``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_brain_surgery(brain):\n",
    "    \"\"\"Helper function to unpack BrainInfo Object.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        brain (BrainInfo) : Single BrainInfo Object\n",
    "    \"\"\"\n",
    "    state = brain.vector_observations[0]\n",
    "    reward = brain.rewards[0]\n",
    "    done = brain.local_done[0]\n",
    "    return (state, reward, done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define the Training Function\n",
    "\n",
    "The cell below trains the agent from scratch and saves the checkpoints when the environment is solved (score > 13). If ``highscore = True``, the agent continues to train until the maximum number of episodes is reached and saves the model whenever a new highscore is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.95, highscore=False):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    max_score = 13                     # keep track of highest winning score\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state, _, done = open_brain_surgery(env.reset(train_mode=True)[brain_name])\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done = open_brain_surgery(env.step(action)[brain_name])\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        avg_score = np.mean(scores_window)\n",
    "        if i_episode % 100 == 0:          # report training progress\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, avg_score))\n",
    "        if math.floor(avg_score)>max_score:\n",
    "            if max_score == 13:           # environment solved\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}\\n'.format(i_episode-100, avg_score))\n",
    "            else:                         # new high score achieved\n",
    "                print('\\nNew high score after {:d} episodes!\\tAverage Score: {:.2f}\\n'.format(i_episode-100, avg_score))\n",
    "            max_score = math.floor(avg_score)\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint_'+flavor+'.pth')\n",
    "            if not highscore:\n",
    "                break\n",
    "    return scores\n",
    "\n",
    "\n",
    "def plot_scores(scores):\n",
    "    \"\"\"Plot Scores.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        scores (list): list of score for each episode\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Agent`` class currently supports three types of agents that can be passed using the ``flavor`` parameter:\n",
    "- ``plain``: Standard DQN agent.\n",
    "- ``double``: Double DQN agent.\n",
    "- ``dueling``: Dueling DQN agent.\n",
    "\n",
    "All agents use a network with two hidden layers. The number of neurons in each layer can be passed in a list to the ``hidden_sizes`` parameter. \n",
    "\n",
    "Let's see how succesful these agents navigate the environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Plain DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0, hidden_sizes = [64, 64], flavor='plain')\n",
    "agent.show_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn()\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0, hidden_sizes = [64, 64], flavor='double')\n",
    "agent.show_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn()\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0, hidden_sizes = [64, 64], flavor='dueling')\n",
    "agent.show_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn()\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Watch the Trained Agents\n",
    "\n",
    "The next cell loads the checkpoint for a trained agent to watch it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "def watch_agent(flavor, file_name = None):\n",
    "    \"\"\"Watch Trained Agent.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    print('Flavor: {}'.format(flavor))\n",
    "    if file_name is None:\n",
    "        file_name = 'checkpoint_'+flavor+'.pth'\n",
    "    \n",
    "    # Load agent and weights\n",
    "    agent = Agent(state_size=state_size, action_size=action_size, seed=0, flavor=flavor)\n",
    "    agent.qnetwork_local.load_state_dict(torch.load(file_name))\n",
    "\n",
    "    # Initiate and run\n",
    "    state, _, done = open_brain_surgery(env.reset(train_mode=False)[brain_name])\n",
    "    score = 0                                          \n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = open_brain_surgery(env.step(action)[brain_name])\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:       \n",
    "            break    \n",
    "    print('Game score: {}'.format(score))\n",
    "    \n",
    "watch_agent('double')\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Possible Extensions\n",
    "\n",
    "Performance and training speed could potentially be increased further:\n",
    "1. The hyperparameters used are likely not optimal. Additional tuning or grid search could be used to make the traing more efficient.\n",
    "2. The current network architecture is surprisingly effective given it's simplicity. However, more elaborate architectures with additional layers of neurons might yield ocnsiderable performance gains.\n",
    "3. There have been many improvements to the DQN algorithm, some of which have been consolidated in the [Rainbow Agent](https://arxiv.org/pdf/1710.02298.pdf). In addition to Dueling and Double DQN implemented here, the following additional improvements:\n",
    "  * The ``ReplayBuffer`` class could be appended to support prioritized experience replay.\n",
    "  * Noisy layers in the network could lead to more efficient exploration.\n",
    "  * Distributional DQN would aim to learn the distribution of rewards instead of the mean. This is presumably less important for this application, as there is relatively little uncertainy in the environment.\n",
    "  * The loss function could be changed to support multi-step learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
